#!/usr/bin/env python3
# coding=utf8

# Copyright  2021  Jiayu DU

import sys
import argparse

DEBUG = None

class Utterance:
    def __init__(self, uid, text):
        self.uid = uid
        self.text = text
        self.tokens = text.split() # white space tokenizer

def GetEditType(ref_token, hyp_token):
    if ref_token == None and hyp_token != None:
        return 'I'
    elif ref_token != None and hyp_token == None:
        return 'D'
    elif ref_token == hyp_token:
        return 'C'
    elif ref_token != hyp_token:
        return 'S'
    else:
        raise RuntimeError

class AlignmentArc:
    def __init__(self, src, dst, ref, hyp):
        self.src = src
        self.dst = dst
        self.ref = ref
        self.hyp = hyp
        self.edit_type = GetEditType(ref, hyp)

def LoadUtterances(filepath):
    utts = {}
    with open(filepath, 'r', encoding='utf8') as f:
        for line in f:
            line = line.strip()
            if line:
                cols = line.split(maxsplit=1)
                assert(len(cols) == 2 or len(cols) == 1)
                uid = cols[0]
                text = cols[1] if len(cols) == 2 else ''
                if utts.get(uid) != None:
                    print(F'found duplicated utterence id {uid}', file = sys.stderr)
                    raise RuntimeError
                utts[uid] = Utterance(uid, text)
    return utts

def similarity_score_function(ref_token, hyp_token):
    return 0 if (ref_token == hyp_token) else -1.0

def insertion_score_function(token):
    return -1.0

def deletion_score_function(token):
    return -1.0

def EditDistance(
        ref,
        hyp, 
        similarity_score_function = similarity_score_function,
        insertion_score_function = insertion_score_function,
        deletion_score_function = deletion_score_function):
    assert(len(ref) != 0)
    class DPState:
        def __init__(self):
            self.score = -float('inf')
            # backpointer
            self.prev_r = None
            self.prev_h = None
    
    def print_search_grid(S, R, H, file_stream):
        print(file=file_stream)
        for r in range(R):
            for h in range(H):
                print(F'[{r},{h}]:{S[r][h].score:4.3f}:({S[r][h].prev_r},{S[r][h].prev_h}) ', end='', file=file_stream)
            print(file=file_stream)

    R = len(ref) + 1
    H = len(hyp) + 1

    # Construct DP search space, a (R x H) grid
    S = [ [] for r in range(R) ]
    for r in range(R):
        S[r] = [ DPState() for x in range(H) ]

    # initialize DP search grid origin, S(r = 0, h = 0)
    S[0][0].score = 0.0
    S[0][0].prev_r = None
    S[0][0].prev_h = None

    # initialize REF axis
    for r in range(1, R):
        S[r][0].score = S[r-1][0].score + deletion_score_function(ref[r-1])
        S[r][0].prev_r = r-1
        S[r][0].prev_h = 0

    # initialize HYP axis
    for h in range(1, H):
        S[0][h].score = S[0][h-1].score + insertion_score_function(hyp[h-1])
        S[0][h].prev_r = 0
        S[0][h].prev_h = h-1

    best_score = S[0][0].score
    best_state = (0, 0)

    for r in range(1, R):
        for h in range(1, H):
            sub_or_cor_score = similarity_score_function(ref[r-1], hyp[h-1])
            new_score = S[r-1][h-1].score + sub_or_cor_score
            if new_score >= S[r][h].score:
                S[r][h].score = new_score
                S[r][h].prev_r = r-1
                S[r][h].prev_h = h-1

            del_score = deletion_score_function(ref[r-1])
            new_score = S[r-1][h].score + del_score
            if new_score >= S[r][h].score:
                S[r][h].score = new_score
                S[r][h].prev_r = r - 1
                S[r][h].prev_h = h

            ins_score = insertion_score_function(hyp[h-1])
            new_score = S[r][h-1].score + ins_score
            if new_score >= S[r][h].score:
                S[r][h].score = new_score
                S[r][h].prev_r = r
                S[r][h].prev_h = h-1

    best_score = S[R-1][H-1].score
    best_state = (R-1, H-1)

    if DEBUG:
        print_search_grid(S, R, H, sys.stderr)

    # Backtracing best alignment path, i.e. a list of arcs
    # arc = (src, dst, ref, hyp, edit_type)
    # src/dst = (r, h), where r/h refers to search grid state-id along Ref/Hyp axis
    best_path = []
    r, h = best_state[0], best_state[1]
    prev_r, prev_h = S[r][h].prev_r, S[r][h].prev_h
    score = S[r][h].score
    # loop invariant:
    #   1. (prev_r, prev_h) -> (r, h) is a "forward arc" on best alignment path
    #   2. score is the value of point(r, h) on DP search grid
    while prev_r != None or prev_h != None:
        src = (prev_r, prev_h)
        dst = (r, h)
        if (r == prev_r + 1 and h == prev_h + 1): # Substitution or correct
            arc = AlignmentArc(src, dst, ref[prev_r], hyp[prev_h])
        elif (r == prev_r + 1 and h == prev_h): # Deletion
            arc = AlignmentArc(src, dst, ref[prev_r], None)
        elif (r == prev_r and h == prev_h + 1): # Insertion
            arc = AlignmentArc(src, dst, None, hyp[prev_h])
        else:
            raise RuntimeError
        best_path.append(arc)
        r, h = prev_r, prev_h
        prev_r, prev_h = S[r][h].prev_r, S[r][h].prev_h
        score = S[r][h].score
    
    best_path.reverse()
    return (best_path, best_score)

def PrettyPrintAlignment(alignment, stream = sys.stderr):
    def get_token_str(token):
        if token == None:
            return "*"
        return token
    
    def is_chinese_char(ch):
        if (ch >= '\u4e00') and (ch <= '\u9fa5'):
            return True
        else:
            return False
    
    def display_width(token_str):
        m = 0
        for c in token_str:
            if is_chinese_char(c):
                m += 2
            else:
                m += 1
        return m

    R = '  REF  : '
    H = '  HYP  : '
    E = '  EDIT : '
    for arc in alignment:
        r = get_token_str(arc.ref)
        h = get_token_str(arc.hyp)
        e = arc.edit_type if arc.edit_type != 'C' else ''

        nr, nh, ne = display_width(r), display_width(h), display_width(e)
        n = max(nr, nh, ne) + 1

        R += r + ' ' * (n-nr)
        H += h + ' ' * (n-nh)
        E += e + ' ' * (n-ne)

    print(R, file=stream)
    print(H, file=stream)
    print(E, file=stream)

def CountEdits(alignment):
    c, s, i, d = 0, 0, 0, 0
    for arc in alignment:
        if arc.edit_type == 'C':
            c += 1
        elif arc.edit_type == 'S':
            s += 1
        elif arc.edit_type == 'I':
            i += 1
        elif arc.edit_type == 'D':
            d += 1
        else:
            raise RuntimeError
    return (c, s, i, d)

def ComputeTokenErrorRate(c, s, i, d):
    return 100.0 * (s + d + i) / (s + d + c)

def ComputeSentenceErrorRate(num_err_utts, num_utts):
    assert(num_utts != 0)
    return 100.0 * num_err_utts / num_utts

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--ref', type=str, required=True, help='reference')
    parser.add_argument('--hyp', type=str, required=True, help='hypothesis')
    parser.add_argument('alignment', type=str)
    args = parser.parse_args()

    ref_utts = LoadUtterances(args.ref); # print(F'{len(ref_utts)} REF utterances loaded.', file=sys.stderr)
    hyp_utts = LoadUtterances(args.hyp); # print(F'{len(hyp_utts)} HYP utterances loaded.', file=sys.stderr)

    utts = []
    num_hyp_without_ref = 0
    for uid in sorted(hyp_utts.keys()):
        if uid in ref_utts.keys(): # TODO: efficiency
            if ref_utts[uid].text.strip(): # non empty reference
                utts.append(uid)
            else:
                print(F'WARNING: found {uid} with empty reference, skipping...', file=sys.stderr)
                continue
        else:
            num_hyp_without_ref += 1
    num_utts = len(utts)
    # print(F'{num_utts} matched REF:HYP utterances.', file=sys.stderr)

    fo = open(args.alignment, 'w+', encoding='utf8')
    C, S, I, D = 0, 0, 0, 0
    num_err_utts = 0
    for uid in utts:
        alignment, score = EditDistance(ref_utts[uid].tokens, hyp_utts[uid].tokens)
        c, s, i, d = CountEdits(alignment)
        utt_ter = ComputeTokenErrorRate(c, s, i, d)
        print(F'{{"uid":{uid}, "score":{score}, "ter":{utt_ter:.2f}, "cor":{c}, "sub":{s}, "ins":{i}, "del":{d}}}', file=fo)
        PrettyPrintAlignment(alignment, fo)

        C += c
        S += s
        I += i
        D += d

        if utt_ter > 0:
            num_err_utts += 1

    ser = ComputeSentenceErrorRate(num_err_utts, num_utts)   
    ter = ComputeTokenErrorRate(C, S, I, D)

    summary = (
        '==================== Overall Statistics ====================\n'
        F'REF: {len(ref_utts)} utterances\n'
        F'HYP: {len(hyp_utts)} utterances\n'
        F'HYP with REF: {num_utts} utterances\n'
        F'HYP without REF: {num_hyp_without_ref} utterances\n'
        F'Sentence Error Rate: {ser:.2f}\n'
        F'Token Error Rate: {ter:.2f}\n'
        F'  - Correct:      {C}\n'
        F'  - Substitution: {S}\n'
        F'  - Insertion:    {I}\n'
        F'  - Deletion:     {D}\n'
        '============================================================\n'
    )
    print(summary, file=fo)
    fo.close()

    ## Kaldi compatible logging
    print(F'%WER {ter:.2f} [ {S + D + I} / {C + S + D}, {I} ins, {D} del, {S} sub ]')
    print(F'%SER {ser:.2f} [ {num_err_utts} / {num_utts} ]')
